{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## problemimiz Aslanları ve kaplanları ayırabilecek bir CNN uygulaması yapmak.\n",
    "## her classın toplam 175 fotoğrafı vardır (toplamda 350) \n",
    "## fotoğrafların 140'i training'e 35'i validation'a ayrılmıştır.\n",
    "## aşağıdaki linkten datalara ulaşabiliriz.\n",
    "## https://drive.google.com/drive/folders/1pzGd11QvxA9F-OHWwPId8-UeWiRCHgD7?usp=sharing\n",
    "\n",
    "#### İLK ADIM ####\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',\n",
    "                       input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation='relu' ))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation='relu' ))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation='relu' ))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def showGraph(history): ## GRAFİK GÖSTERME ÇOK KULLANACAĞIMIZDAN FONSKİYON OLARAK YAZDIK\n",
    "    plt.axis([-1, 20, 0, 1.1])\n",
    "\n",
    "\n",
    "    plt.xticks(np.arange(0,21, 5.0))\n",
    "\n",
    "\n",
    "    acc=history.history['acc']\n",
    "    val_acc=history.history['val_acc']\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']\n",
    "\n",
    "    epochs=range(len(acc))\n",
    "\n",
    "\n",
    "    plt.plot(epochs,acc,'bo',label='Training_acc')\n",
    "    plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.axis([-1, 20, 0, 1.5])\n",
    "    plt.xticks(np.arange(0,21, 5.0))\n",
    "    plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "    plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Berke\\Desktop\\ae\n",
      "Found 280 images belonging to 2 classes.\n",
      "Found 70 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    " \n",
    "\n",
    "train_dir= os.getcwd() + r'\\FinalData\\train'\n",
    "train_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory(\n",
    "                             train_dir,\n",
    "target_size=(150,150),\n",
    "batch_size=5,\n",
    "class_mode='binary')\n",
    "\n",
    "validation_dir= os.getcwd() + r'\\FinalData\\validation'\n",
    "validation_generator=test_datagen.flow_from_directory(\n",
    "validation_dir,\n",
    "target_size=(150,150),\n",
    "batch_size=5,\n",
    "class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.5611 - acc: 0.6920 - val_loss: 0.2144 - val_acc: 0.9320\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 13s 132ms/step - loss: 0.3657 - acc: 0.8560 - val_loss: 0.5086 - val_acc: 0.9200\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.2998 - acc: 0.8800 - val_loss: 0.2096 - val_acc: 0.9280\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 13s 131ms/step - loss: 0.2438 - acc: 0.9080 - val_loss: 0.1571 - val_acc: 0.7120\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2460 - acc: 0.9040 - val_loss: 0.0810 - val_acc: 0.9400\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 13s 134ms/step - loss: 0.2119 - acc: 0.9140 - val_loss: 0.9226 - val_acc: 0.8480\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.1926 - acc: 0.9280 - val_loss: 0.0466 - val_acc: 0.9600\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.1525 - acc: 0.9380 - val_loss: 0.0080 - val_acc: 0.9040\n",
      "Epoch 9/20\n",
      " 92/100 [==========================>...] - ETA: 0s - loss: 0.1557 - acc: 0.9261"
     ]
    }
   ],
   "source": [
    "firstHistory=model.fit_generator( ## NORMAL FİT\n",
    "train_generator,\n",
    "steps_per_epoch=100,\n",
    "epochs=20,\n",
    "validation_data=validation_generator,\n",
    "validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "showGraph(firstHistory)  ## NORMAL GRAFİK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YORUMLAR\n",
    "print(\"Accuracy çok yüksek ve epoch ilerledikçe accuracy'miz yükseliyor, burası iyi\")\n",
    "print(\"Training loss genel olarak epoch boyunca düşmeye gidiyor ve sonlara doğru stabilize oluyor,istediğimiz gibi grafik çiziyor\")\n",
    "print(\"Ama Validation loss Training loss paralel değil bazı noktalarda sıçrama yapıyor, daha iyi olabilirdi\")\n",
    "print(\"Bu duruma, araştırdığıma göre \\\"Unrepresentative Validation Dataset\\\" deniyor. \")\n",
    "print(\"Ve bu durum aşağıdaki sebepten ötürü oluşuyor.\")\n",
    "print(\"Validation dataseti train data setine göre küçük olduğundan veya o datasete göre tahmin etmesi daha kolay olduğundan bu sıçramalara sebep olabilir\")\n",
    "print(\"Kaynak: https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### İKİNCİ ADIM ####\n",
    "\n",
    "aug_train_datagen=ImageDataGenerator(  ## AUGMENT YAPIYORUZ\n",
    "rescale=1./255,\n",
    "rotation_range=40,\n",
    "width_shift_range=.2,\n",
    "height_shift_range=.2,\n",
    "shear_range=.2,\n",
    "zoom_range=.2,\n",
    "horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=aug_train_datagen.flow_from_directory(\n",
    "                             train_dir,\n",
    "target_size=(150,150),\n",
    "batch_size=5,\n",
    "class_mode='binary')\n",
    "\n",
    "\n",
    "validation_generator=test_datagen.flow_from_directory(\n",
    "validation_dir,\n",
    "target_size=(150,150),\n",
    "batch_size=5,\n",
    "class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondHistory=model.fit_generator( ## AUGMENT FİT\n",
    "train_generator,\n",
    "steps_per_epoch=100,\n",
    "epochs=20,\n",
    "validation_data=validation_generator,\n",
    "validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph(secondHistory) ## AUGMENT GRAFİĞİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YORUMLAR\n",
    "print(\"Modelimiz Augment edilen dataları tahmin etmekte birazcık daha zorlandı diyebiliriz (accuracy biraz düştü)\")\n",
    "print(\"Ama validation acc ve accuracy daha paralel görünüyor\")\n",
    "print(\"Training loss miktar olarak yükseldi ancak eğrisi hala aşağı gidiyor\")\n",
    "print(\"Augment edilmiş datalara rağmen Unrepresentative Validation Dataset durumu geçmedi ama iyileşti\")\n",
    "print(\"Validation loss bir yer hariç sıçramalarında aralığında azalma var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ÜÇÜNCÜ ADIM ####\n",
    "\n",
    "model=models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',\n",
    "                       input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation='relu' ))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation='relu' ))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation='relu' ))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(.5))                            ##   MODELE DROPOUT EKLEDİK\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdHistory=model.fit_generator(  ## AUGMENT + DROP OUT FİT\n",
    "train_generator,\n",
    "steps_per_epoch=100,\n",
    "epochs=20,\n",
    "validation_data=validation_generator,\n",
    "validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph(thirdHistory) ## AUGMENT + DROP OUT GRAFİĞİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YORUMLAR\n",
    "print(\"Drop out modelin Accuracy olarak düşmesine sebep oldu\")\n",
    "print(\"Training loss eğrisi aşağı yönde\")\n",
    "print(\"Dropout ile Unrepresentative Validation Dataset durumu geçmedi ama daha da iyileşti\")\n",
    "print(\"Bu yüzden Validation loss'ta hala sıçramalar var ama önceki grafiklere göre daha iyi duruyor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### DÖRDÜNCÜ ADIM ####\n",
    "\n",
    "fourthHistory=model.fit_generator( ## AUGMENT + DROP OUT + ERKEN EPOCH'TA KESME FİT\n",
    "train_generator,\n",
    "steps_per_epoch=100,\n",
    "epochs=10,                         ## 10. EPOCH'DA KESTİK\n",
    "validation_data=validation_generator,\n",
    "validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph(fourthHistory) ## AUGMENT + DROP OUT + ERKEN EPOCH'TA KESME GRAFİĞİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YORUMLAR\n",
    "print(\"Epoch'umuzu 20 den 10 a indirince validation loss çok daha stabil bir eğri çizdi\")\n",
    "print(\"Accuracy'miz hala yüksek\")\n",
    "print(\"Ama training loss ile paralel değil, dolayısıyla modelimiz good fit demek demek tam doğru olmayabilir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
